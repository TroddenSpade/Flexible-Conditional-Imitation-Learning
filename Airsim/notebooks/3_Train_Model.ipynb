{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.Train-Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "9lntZv6vSLD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYd0OGjyHqeT"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/Shareddrives/Sam/Datasets/NH/images.npy .\n",
        "!cp /content/drive/Shareddrives/Sam/Datasets/NH/controls.npy .\n",
        "!cp /content/drive/Shareddrives/Sam/Datasets/NH/points.npy .\n",
        "!cp /content/drive/Shareddrives/Sam/Datasets/NH/labels.npy ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images = np.load('images.npy')\n",
        "controls = np.load('controls.npy')\n",
        "points = np.load('points.npy')\n",
        "labels = np.load('labels.npy')"
      ],
      "metadata": {
        "id": "Tbu__UY-zc2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images.shape, controls.shape, points.shape, labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlARJAUcTf4c",
        "outputId": "e8c5f2f0-b080-4a84-9f56-8657b7d985ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((34437, 70, 254, 3), (34437, 4), (34437, 20), (34437, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indx = 480\n",
        "image, ctrls, pts, lbl = images[indx], controls[indx], points[indx], labels[indx]"
      ],
      "metadata": {
        "id": "NO6sSy7mz6tE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (pt_ax, im_ax, ctrl_ax, lbl_ax) = plt.subplots(1, 4, figsize=(18, 5))\n",
        "pt_ax.scatter(pts[::2], pts[1::2], s=5)\n",
        "pt_ax.set_xlim(0, 1)\n",
        "pt_ax.set_ylim(0.5, -0.5)\n",
        "\n",
        "im_ax.imshow(image)\n",
        "  \n",
        "fields = ['Speed', 'Throttle', 'Steering', 'Brake']\n",
        "ctrl_ax.bar(fields, ctrls)\n",
        "ctrl_ax.set_ylim(0, 1)\n",
        "\n",
        "lbl_ax.bar(fields[1:], lbl)\n",
        "lbl_ax.set_ylim(0, 1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "ZyL72T5n0CYr",
        "outputId": "118e315c-71ae-4770-cd12-d6fb364a892c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1296x360 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size = images.shape[0]\n",
        "mask = np.zeros((size,), dtype=bool)\n",
        "np.random.seed(0)\n",
        "indx = np.random.choice(size, size//6)\n",
        "mask[indx] = True"
      ],
      "metadata": {
        "id": "xxrZfO8j9N1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "SHUFFLE_BUFFER_SIZE = 128\n",
        "\n",
        "def normalize_image(image, control, point, label):\n",
        "  return tf.cast(image, tf.float32) / 255., control, point, label\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    images[mask], controls[mask], points[mask], labels[mask]))\\\n",
        "    .map(normalize_image)\\\n",
        "    .shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "    \n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    images[~mask], controls[~mask], points[~mask], labels[~mask]))\\\n",
        "    .map(normalize_image)\\\n",
        "    .shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "zyLxGs35SGxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset), len(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rFceZq-ASp7",
        "outputId": "fed1a89d-970e-49ef-cfec-f28e5bcb4b20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(228, 42)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class Model(tf.keras.Model):\n",
        "\n",
        "#   def __init__(self):\n",
        "#     super().__init__()\n",
        "#     self.input_img = None\n",
        "#     self.conv1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
        "#     self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
        "#     self.dropout = tf.keras.layers.Dropout(0.5)\n",
        "\n",
        "#   def call(self, inputs, training=False):\n",
        "#     x = self.dense1(inputs)\n",
        "#     if training:\n",
        "#       x = self.dropout(x, training=training)\n",
        "#     return self.dense2(x)\n",
        "\n",
        "# model = MyModel()"
      ],
      "metadata": {
        "id": "N8TLEn_jDBz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 1"
      ],
      "metadata": {
        "id": "KtYA1CJK89nR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input_img = tf.keras.Input(shape=(70, 254, 3))\n",
        "\n",
        "# conv1 = tf.keras.layers.Conv2D(32, (5,5), strides=(1,1), padding='valid')\n",
        "# b1 = tf.keras.layers.BatchNormalization()\n",
        "# d1 = tf.keras.layers.Dropout(0.5)\n",
        "# a1 = tf.keras.layers.ReLU()\n",
        "# max1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='valid')\n",
        "\n",
        "# conv2 = tf.keras.layers.Conv2D(64, (3,3), strides=(1,1), padding='valid')\n",
        "# b2 = tf.keras.layers.BatchNormalization()\n",
        "# d2 = tf.keras.layers.Dropout(0.5)\n",
        "# a2 = tf.keras.layers.ReLU()\n",
        "# max2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='valid')\n",
        "\n",
        "# conv3 = tf.keras.layers.Conv2D(128, (3,3), strides=(1,1), padding='valid')\n",
        "# b3 = tf.keras.layers.BatchNormalization()\n",
        "# d3 = tf.keras.layers.Dropout(0.5)\n",
        "# a3 = tf.keras.layers.ReLU()\n",
        "# max3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='valid')\n",
        "\n",
        "# conv4 = tf.keras.layers.Conv2D(256, (3,3), strides=(1,1), padding='valid')\n",
        "# b4 = tf.keras.layers.BatchNormalization()\n",
        "# d4 = tf.keras.layers.Dropout(0.5)\n",
        "# a4 = tf.keras.layers.ReLU()\n",
        "# max4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='valid')\n",
        "\n",
        "# img_stack = tf.keras.layers.Flatten()\n",
        "\n",
        "# dense_img1 = tf.keras.layers.Dense(512, activation='relu')\n",
        "# dense_img2 = tf.keras.layers.Dense(256, activation='relu')\n",
        "\n",
        "# input_ctrl = tf.keras.Input(shape=(4,))\n",
        "# dense_ctrl1 = tf.keras.layers.Dense(32, activation='relu')\n",
        "# dense_ctrl2 = tf.keras.layers.Dense(64, activation='relu')\n",
        "\n",
        "\n",
        "# input_pts = tf.keras.Input(shape=(20,))\n",
        "# dense_pts1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "# dense_pts2 = tf.keras.layers.Dense(128, activation='relu')\n",
        "\n",
        "# merge = tf.keras.layers.Concatenate()\n",
        "\n",
        "# dense_merge1 = tf.keras.layers.Dense(128, activation='relu')\n",
        "# dense_merge2 = tf.keras.layers.Dense(32, activation='relu')\n",
        "# dense_merge3 = tf.keras.layers.Dense(3, activation='relu')\n",
        "\n",
        "# # dropout = tf.keras.layers.Dropout(0.5)"
      ],
      "metadata": {
        "id": "aOU1p2yfQ6F-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_img = conv1(input_img)\n",
        "# x_img = b1(x_img)\n",
        "# x_img = a1(x_img)\n",
        "# x_img = max1(x_img)\n",
        "\n",
        "# x_img = conv2(x_img)\n",
        "# x_img = b2(x_img)\n",
        "# x_img = a2(x_img)\n",
        "# x_img = max2(x_img)\n",
        "\n",
        "# x_img = conv3(x_img)\n",
        "# x_img = b3(x_img)\n",
        "# x_img = a3(x_img)\n",
        "# x_img = max3(x_img)\n",
        "\n",
        "# x_img = conv4(x_img)\n",
        "# x_img = b4(x_img)\n",
        "# x_img = a4(x_img)\n",
        "# x_img = max4(x_img)\n",
        "\n",
        "# x_img = img_stack(x_img)\n",
        "\n",
        "# x_img = dense_img1(x_img)\n",
        "# x_img = dense_img2(x_img)\n",
        "\n",
        "# x_ctrl = dense_ctrl1(input_ctrl)\n",
        "# x_ctrl = dense_ctrl2(x_ctrl)\n",
        "\n",
        "# x_pts = dense_pts1(input_pts)\n",
        "# x_pts = dense_pts2(x_pts)\n",
        "\n",
        "# x = merge([x_img, x_ctrl, x_pts])\n",
        "\n",
        "# x = dense_merge1(x)\n",
        "# x = dense_merge2(x)\n",
        "# x = dense_merge3(x)\n",
        "# x = tf.keras.activations.sigmoid(x)\n",
        "\n",
        "# model = tf.keras.Model([input_img, input_ctrl, input_pts], x)\n",
        "# opt = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
        "# model.compile(optimizer=opt, loss='mse')"
      ],
      "metadata": {
        "id": "NJFcWfz3hgcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 2"
      ],
      "metadata": {
        "id": "Lc87b75aDtTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# activation = 'relu'\n",
        "# input_img = tf.keras.Input(shape=(70, 254, 3))\n",
        "\n",
        "# img_stack = tf.keras.layers.Conv2D(16, (3, 3), padding='same', activation=activation)(input_img)\n",
        "# img_stack = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(img_stack)\n",
        "# img_stack = tf.keras.layers.Conv2D(32, (3, 3), activation=activation, padding='same')(img_stack)\n",
        "# img_stack = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(img_stack)\n",
        "# img_stack = tf.keras.layers.Conv2D(32, (3, 3), activation=activation, padding='same')(img_stack)\n",
        "# img_stack = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(img_stack)\n",
        "# img_stack = tf.keras.layers.Flatten()(img_stack)\n",
        "# img_stack = tf.keras.layers.Dropout(0.2)(img_stack)\n",
        "\n",
        "# ###############\n",
        "# input_pts = tf.keras.Input(shape=(20,))\n",
        "# dense_pts1 = tf.keras.layers.Dense(16, activation='relu')(input_pts)\n",
        "# dense_pts2 = tf.keras.layers.Dense(16, activation='relu')(dense_pts1)\n",
        "\n",
        "# #Inject the state input\n",
        "# input_ctrl = tf.keras.Input(shape=(4,))\n",
        "# merged = tf.keras.layers.concatenate([img_stack, input_ctrl, dense_pts2])\n",
        "\n",
        "# # Add a few dense layers to finish the model\n",
        "# merged = tf.keras.layers.Dense(64, activation=activation)(merged)\n",
        "# merged = tf.keras.layers.Dropout(0.2)(merged)\n",
        "# merged = tf.keras.layers.Dense(10, activation=activation)(merged)\n",
        "# merged = tf.keras.layers.Dropout(0.2)(merged)\n",
        "# merged = tf.keras.layers.Dense(3, name='output')(merged)\n",
        "\n",
        "# adam = tf.keras.optimizers.Nadam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "# model = tf.keras.Model(inputs=[input_img, input_ctrl, input_pts], outputs=merged)\n",
        "# model.compile(optimizer=adam, loss='mse')"
      ],
      "metadata": {
        "id": "hu2PY0aatFGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.summary()"
      ],
      "metadata": {
        "id": "QB7WljLNh5bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dot_img_file = './model_1.png'\n",
        "# tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)"
      ],
      "metadata": {
        "id": "2_9iujsQ1Nj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model 3 VGG-based\n"
      ],
      "metadata": {
        "id": "1pdFKb8l-lEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# activation = 'relu'\n",
        "# input_img = tf.keras.Input(shape=(70, 254, 3))\n",
        "\n",
        "# img_stack = tf.keras.layers.Conv2D(16, (3, 3), padding='same', activation=activation)(input_img) #32 from start, 5*5 kernel\n",
        "# img_stack = tf.keras.layers.Conv2D(16, (3, 3), padding='same', activation=activation)(input_img)\n",
        "# img_stack = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(img_stack)\n",
        "\n",
        "# img_stack = tf.keras.layers.Conv2D(32, (3, 3), activation=activation, padding='same')(img_stack)\n",
        "# img_stack = tf.keras.layers.Conv2D(32, (3, 3), activation=activation, padding='same')(img_stack)\n",
        "# img_stack = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(img_stack)\n",
        "\n",
        "# img_stack = tf.keras.layers.Conv2D(64, (3, 3), activation=activation, padding='same')(img_stack)\n",
        "# img_stack = tf.keras.layers.Conv2D(64, (3, 3), activation=activation, padding='same')(img_stack)\n",
        "# img_stack = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(img_stack)\n",
        "\n",
        "# img_stack = tf.keras.layers.Conv2D(128, (3, 3), activation=activation, padding='same')(img_stack)\n",
        "# img_stack = tf.keras.layers.Conv2D(128, (3, 3), activation=activation, padding='same')(img_stack)\n",
        "# img_stack = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(img_stack)\n",
        "\n",
        "# img_stack = tf.keras.layers.Flatten()(img_stack)\n",
        "# img_stack = tf.keras.layers.Dropout(0.2)(img_stack)\n",
        "\n",
        "# dense_img1 = tf.keras.layers.Dense(1024, activation='relu')(img_stack)\n",
        "# dense_img1 = tf.keras.layers.Dropout(0.2)(dense_img1)\n",
        "# dense_img2 = tf.keras.layers.Dense(512, activation='relu')(dense_img1)\n",
        "# dense_img2 = tf.keras.layers.Dropout(0.2)(dense_img2)\n",
        "\n",
        "\n",
        "# ######################################\n",
        "# input_pts = tf.keras.Input(shape=(20,))\n",
        "# dense_pts1 = tf.keras.layers.Dense(32, activation='relu')(input_pts) # 64 , 32 , 16\n",
        "# dense_pts2 = tf.keras.layers.Dense(32, activation='relu')(dense_pts1)\n",
        "\n",
        "# ######################################\n",
        "# input_ctrl = tf.keras.Input(shape=(4,))\n",
        "# merged = tf.keras.layers.concatenate([dense_img2, input_ctrl, dense_pts2])\n",
        "\n",
        "# # Add a few dense layers to finish the model\n",
        "# merged = tf.keras.layers.Dense(200, activation=activation)(merged)\n",
        "# merged = tf.keras.layers.Dropout(0.2)(merged)\n",
        "# merged = tf.keras.layers.Dense(64, activation=activation)(merged)\n",
        "# merged = tf.keras.layers.Dropout(0.2)(merged)\n",
        "# merged = tf.keras.layers.Dense(3, name='output')(merged)\n",
        "\n",
        "# adam = tf.keras.optimizers.Nadam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "# model3 = tf.keras.Model(inputs=[input_img, input_ctrl, input_pts], outputs=merged)\n",
        "# model3.compile(optimizer=adam, loss='mse')"
      ],
      "metadata": {
        "id": "lAfsPCSb-kEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model3.summary()"
      ],
      "metadata": {
        "id": "ZNAfB7mf_Nni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 4"
      ],
      "metadata": {
        "id": "KJPqN4pqumSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 - no dropout & no bn\n",
        "# 1 - bn before activation layer + dropout after maxpool\n",
        "# 2 - bn after each layer's activation + dropout after maxpool\n",
        "# 3 - bn before maxpool + dropout after maxpool\n",
        "# 4 - only dropout\n",
        "\n",
        "def conv2d_block(x, n_layer=2, n_channel=16, kernel=(3,3), t=1):\n",
        "    for i in range(n_layer):\n",
        "        x = tf.keras.layers.Conv2D(n_channel, kernel, padding='same')(x)\n",
        "        if t == 1:\n",
        "            x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Activation('relu')(x)\n",
        "        if t == 2:\n",
        "            x = tf.keras.layers.BatchNormalization()(x)\n",
        "    if t == 3:\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(x)\n",
        "    if t != 0:\n",
        "        x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "VSBfp4IHnE4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 - no dropout\n",
        "# 1 - dropout after activation layer\n",
        "\n",
        "def fc_block(x, size, t=1):\n",
        "    x = tf.keras.layers.Dense(size)(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    if t != 0:\n",
        "        x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "b6MQJPFdZpWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_img = tf.keras.Input(shape=(70, 254, 3))\n",
        "\n",
        "img_stack = conv2d_block(input_img, 2, 16, (3,3), t=1)\n",
        "img_stack = conv2d_block(img_stack, 2, 32, (3,3), t=1)\n",
        "img_stack = conv2d_block(img_stack, 2, 64, (3,3), t=1)\n",
        "img_stack = conv2d_block(img_stack, 2, 128, (3,3), t=1)\n",
        "\n",
        "img_stack = tf.keras.layers.Flatten()(img_stack)\n",
        "img_stack = tf.keras.layers.Dropout(0.2)(img_stack)\n",
        "\n",
        "dense_img = fc_block(img_stack, 1024)\n",
        "dense_img = fc_block(dense_img, 512)\n",
        "\n",
        "\n",
        "######################################\n",
        "input_pts = tf.keras.Input(shape=(20,))\n",
        "dense_pts = fc_block(input_pts, 32)\n",
        "dense_pts = fc_block(dense_pts, 32)\n",
        "\n",
        "######################################\n",
        "input_ctrl = tf.keras.Input(shape=(4,))\n",
        "dense_ctrl = fc_block(input_ctrl, 8)\n",
        "dense_ctrl = fc_block(dense_ctrl, 16)###############\n",
        "merged = tf.keras.layers.concatenate([dense_img, dense_ctrl, dense_pts])\n",
        "\n",
        "# Add a few dense layers to finish the model\n",
        "merged = fc_block(merged, 200)\n",
        "merged = fc_block(merged, 64)\n",
        "merged = tf.keras.layers.Dense(3, name='output')(merged)\n",
        "### sigmoid layer\n",
        "\n",
        "adam = tf.keras.optimizers.Nadam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "model4 = tf.keras.Model(inputs=[input_img, input_ctrl, input_pts], outputs=merged)\n",
        "model4.compile(optimizer=adam, loss='mse')"
      ],
      "metadata": {
        "id": "men_zMw7tgjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVYy93q_9F4i",
        "outputId": "31c6bd4c-0d85-4f17-fd5e-edea52ab26f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 70, 254, 3)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 70, 254, 16)  448         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 70, 254, 16)  64         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 70, 254, 16)  0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 70, 254, 16)  2320        ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 70, 254, 16)  64         ['conv2d_1[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 70, 254, 16)  0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 35, 127, 16)  0           ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 35, 127, 16)  0           ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 35, 127, 32)  4640        ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 35, 127, 32)  128        ['conv2d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 35, 127, 32)  0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 35, 127, 32)  9248        ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 35, 127, 32)  128        ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 35, 127, 32)  0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 17, 63, 32)  0           ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 17, 63, 32)   0           ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 17, 63, 64)   18496       ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 17, 63, 64)  256         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 17, 63, 64)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 17, 63, 64)   36928       ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 17, 63, 64)  256         ['conv2d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 17, 63, 64)   0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 8, 31, 64)   0           ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 8, 31, 64)    0           ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 8, 31, 128)   73856       ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 8, 31, 128)  512         ['conv2d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 8, 31, 128)   0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 8, 31, 128)   147584      ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 8, 31, 128)  512         ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 8, 31, 128)   0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPooling2D)  (None, 4, 15, 128)  0           ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 4, 15, 128)   0           ['max_pooling2d_3[0][0]']        \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 7680)         0           ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 7680)         0           ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 4)]          0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 20)]         0           []                               \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 1024)         7865344     ['dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 8)            40          ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 32)           672         ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1024)         0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 8)            0           ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 32)           0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 1024)         0           ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 8)            0           ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 32)           0           ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 512)          524800      ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 16)           144         ['dropout_9[0][0]']              \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 32)           1056        ['dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 512)          0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 16)           0           ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 32)           0           ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 512)          0           ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 16)           0           ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 32)           0           ['activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 560)          0           ['dropout_6[0][0]',              \n",
            "                                                                  'dropout_10[0][0]',             \n",
            "                                                                  'dropout_8[0][0]']              \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 200)          112200      ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 200)          0           ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, 200)          0           ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 64)           12864       ['dropout_11[0][0]']             \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 64)           0           ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 64)           0           ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " output (Dense)                 (None, 3)            195         ['dropout_12[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,812,755\n",
            "Trainable params: 8,811,795\n",
            "Non-trainable params: 960\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir saved_model"
      ],
      "metadata": {
        "id": "rrLQEB2bRVcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 500\n",
        "train_loss_values = np.zeros((epochs,))\n",
        "test_loss_values = np.zeros((epochs,))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    loss = 0\n",
        "    for step, (imgs, ctrls, pts, lbls) in enumerate(train_dataset):\n",
        "        loss = model4.train_on_batch([imgs, ctrls, pts], lbls)\n",
        "        train_loss_values[epoch] += loss\n",
        "        if step % 10 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss))\n",
        "            )\n",
        "            print(\"Seen so far: %s samples\" % ((step + 1) * BATCH_SIZE))\n",
        "\n",
        "    for step, (imgs, ctrls, pts, lbls) in enumerate(test_dataset):\n",
        "        loss = model4.test_on_batch([imgs, ctrls, pts], lbls)\n",
        "        test_loss_values[epoch] += loss\n",
        "\n",
        "    if epoch % 10 == 9:\n",
        "        model4.save('./saved_model/model-' + str(epoch))\n",
        "    print(\"\\nLoss: %f, Train Loss: %f\" % (train_loss_values[epoch], test_loss_values[epoch]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvUI38PSh6bj",
        "outputId": "19f15404-97be-4694-f4df-251cbd1dcaff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training loss (for one batch) at step 40: 0.0124\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0122\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0083\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0151\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0234\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0084\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0101\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0060\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0077\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0101\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0112\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0058\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0141\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0098\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0133\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0055\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0067\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0117\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0094\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.017242, Train Loss: 0.858888\n",
            "\n",
            "Start of epoch 401\n",
            "Training loss (for one batch) at step 0: 0.0009\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0004\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0018\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0008\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0161\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0129\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0067\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0169\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0136\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0066\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0111\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0096\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0097\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0068\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0186\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0090\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0079\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0067\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0091\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0081\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0088\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0155\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0147\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.083946, Train Loss: 0.793789\n",
            "\n",
            "Start of epoch 402\n",
            "Training loss (for one batch) at step 0: 0.0026\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0010\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0030\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0014\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0162\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0131\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0215\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0148\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0124\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0073\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0135\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0061\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0120\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0070\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0085\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0186\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0085\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0070\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0100\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0062\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0071\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0154\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0167\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.149734, Train Loss: 0.823043\n",
            "\n",
            "Start of epoch 403\n",
            "Training loss (for one batch) at step 0: 0.0020\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0006\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0010\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0008\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0188\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0131\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0136\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0156\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0152\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0058\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0108\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0065\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0113\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0066\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0079\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0071\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0097\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0068\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0077\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0071\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0105\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0116\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0108\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.988030, Train Loss: 0.798830\n",
            "\n",
            "Start of epoch 404\n",
            "Training loss (for one batch) at step 0: 0.0020\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0008\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0012\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0009\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0133\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0128\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0094\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0190\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0165\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0165\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0217\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0059\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0110\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0112\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0080\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0118\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0097\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0094\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0082\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0069\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0108\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0097\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0086\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.022544, Train Loss: 0.819262\n",
            "\n",
            "Start of epoch 405\n",
            "Training loss (for one batch) at step 0: 0.0016\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0005\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0011\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0004\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0183\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0116\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0111\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0143\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0202\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0071\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0083\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0058\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0090\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0066\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0109\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0087\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0127\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0163\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0088\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0083\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0103\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0182\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0115\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.049878, Train Loss: 0.787001\n",
            "\n",
            "Start of epoch 406\n",
            "Training loss (for one batch) at step 0: 0.0018\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0011\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0017\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0006\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0161\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0214\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0098\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0148\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0192\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0169\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0094\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0055\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0106\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0237\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0086\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0064\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0167\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0063\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0117\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0053\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0099\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0226\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0136\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.103815, Train Loss: 0.799451\n",
            "\n",
            "Start of epoch 407\n",
            "Training loss (for one batch) at step 0: 0.0041\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0010\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0018\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0008\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0151\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0148\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0126\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0177\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0183\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0089\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0097\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0047\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0115\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0069\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0121\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0096\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0197\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0049\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0078\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0050\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0085\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0093\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0192\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.119636, Train Loss: 0.780488\n",
            "\n",
            "Start of epoch 408\n",
            "Training loss (for one batch) at step 0: 0.0012\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0006\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0024\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0005\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0123\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0136\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0116\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0190\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0145\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0109\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0105\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0056\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0107\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0050\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0094\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0115\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0388\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0097\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0072\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0053\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0076\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0099\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0100\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.044250, Train Loss: 0.827484\n",
            "\n",
            "Start of epoch 409\n",
            "Training loss (for one batch) at step 0: 0.0012\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0012\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0013\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0005\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0149\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0140\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0105\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0200\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0109\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0086\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0109\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0070\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0094\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0067\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0069\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0070\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0090\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0073\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0092\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0047\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0101\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0090\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0194\n",
            "Seen so far: 28288 samples\n",
            "INFO:tensorflow:Assets written to: ./saved_model/model-409/assets\n",
            "\n",
            "Loss: 2.062905, Train Loss: 0.754559\n",
            "\n",
            "Start of epoch 410\n",
            "Training loss (for one batch) at step 0: 0.0010\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0011\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0012\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0009\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0154\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0178\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0106\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0166\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0129\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0078\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0125\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0072\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0086\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0075\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0121\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0088\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0091\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0096\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0078\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0076\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0104\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0114\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0102\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.055428, Train Loss: 0.774966\n",
            "\n",
            "Start of epoch 411\n",
            "Training loss (for one batch) at step 0: 0.0043\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0011\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0014\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0005\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0152\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0138\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0186\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0195\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0144\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0062\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0093\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0061\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0188\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0054\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0196\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0091\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0090\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0065\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0102\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0060\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0087\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0076\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0084\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.011184, Train Loss: 0.836213\n",
            "\n",
            "Start of epoch 412\n",
            "Training loss (for one batch) at step 0: 0.0015\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0014\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0017\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0011\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0193\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0121\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0115\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0126\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0288\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0049\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0102\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0052\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0209\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0120\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0115\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0215\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0121\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0061\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0172\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0060\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0102\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0107\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0163\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.008155, Train Loss: 0.774682\n",
            "\n",
            "Start of epoch 413\n",
            "Training loss (for one batch) at step 0: 0.0008\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0018\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0013\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0015\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0243\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0139\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0129\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0157\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0185\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0089\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0113\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0053\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0107\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0093\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0092\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0061\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0109\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0076\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0183\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0080\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0123\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0066\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0075\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.006766, Train Loss: 0.774759\n",
            "\n",
            "Start of epoch 414\n",
            "Training loss (for one batch) at step 0: 0.0012\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0010\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0016\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0007\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0263\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0153\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0110\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0267\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0187\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0070\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0098\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0065\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0087\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0092\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0126\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0090\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0099\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0088\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0105\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0084\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0085\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0105\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0131\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.028501, Train Loss: 0.794294\n",
            "\n",
            "Start of epoch 415\n",
            "Training loss (for one batch) at step 0: 0.0016\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0018\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0008\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0013\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0236\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0086\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0100\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0171\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0150\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0090\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0107\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0062\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0144\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0082\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0115\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0076\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0117\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0110\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0081\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0073\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0075\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0120\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0183\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.152305, Train Loss: 0.758573\n",
            "\n",
            "Start of epoch 416\n",
            "Training loss (for one batch) at step 0: 0.0012\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0011\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0014\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0010\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0150\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0123\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0090\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0151\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0313\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0077\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0081\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0065\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0124\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0091\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0093\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0061\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0083\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0061\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0098\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0057\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0104\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0118\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0107\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.045889, Train Loss: 0.807277\n",
            "\n",
            "Start of epoch 417\n",
            "Training loss (for one batch) at step 0: 0.0013\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0012\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0021\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0012\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0148\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0107\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0245\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0196\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0122\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0079\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0218\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0061\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0087\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0082\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0082\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0078\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0108\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0072\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0092\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0055\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0201\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0100\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0075\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.069451, Train Loss: 0.814125\n",
            "\n",
            "Start of epoch 418\n",
            "Training loss (for one batch) at step 0: 0.0010\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0006\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0021\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0009\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0205\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0136\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0057\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0175\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0160\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0062\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0137\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0045\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0097\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0107\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0062\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0152\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0104\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0073\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0072\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0068\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0078\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0194\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0059\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.018625, Train Loss: 0.813331\n",
            "\n",
            "Start of epoch 419\n",
            "Training loss (for one batch) at step 0: 0.0021\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0006\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0012\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0012\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0167\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0148\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0136\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0179\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0146\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0093\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0079\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0051\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0165\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0054\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0083\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0168\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0077\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0060\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0102\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0062\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0064\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0084\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0078\n",
            "Seen so far: 28288 samples\n",
            "INFO:tensorflow:Assets written to: ./saved_model/model-419/assets\n",
            "\n",
            "Loss: 2.004677, Train Loss: 0.837695\n",
            "\n",
            "Start of epoch 420\n",
            "Training loss (for one batch) at step 0: 0.0011\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0010\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0017\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0008\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0219\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0123\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0111\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0192\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0194\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0074\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0116\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0060\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0087\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0049\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0087\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0080\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0099\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0078\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0085\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0073\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0122\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0099\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0084\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.059673, Train Loss: 0.814909\n",
            "\n",
            "Start of epoch 421\n",
            "Training loss (for one batch) at step 0: 0.0009\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0018\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0013\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0007\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0139\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0137\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0086\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0142\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0125\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0075\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0091\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0037\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0083\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0144\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0087\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0077\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0098\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0059\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0081\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0073\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0079\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0117\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0107\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.029415, Train Loss: 0.816343\n",
            "\n",
            "Start of epoch 422\n",
            "Training loss (for one batch) at step 0: 0.0014\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0007\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0012\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0008\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0135\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0160\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0100\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0155\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0181\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0068\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0078\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0041\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0083\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0068\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0071\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0062\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0110\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0107\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0115\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0052\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0110\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0111\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0109\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.092121, Train Loss: 0.815488\n",
            "\n",
            "Start of epoch 423\n",
            "Training loss (for one batch) at step 0: 0.0014\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0017\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0012\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0008\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0190\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0241\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0136\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0213\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0188\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0079\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0097\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0103\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0144\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0073\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0219\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0069\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0130\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0108\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0065\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0085\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0105\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0099\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0120\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.063657, Train Loss: 0.780913\n",
            "\n",
            "Start of epoch 424\n",
            "Training loss (for one batch) at step 0: 0.0007\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0007\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0011\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0010\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0211\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0117\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0124\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0184\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0193\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0048\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0320\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0075\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0071\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0063\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0184\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0079\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0175\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0177\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0110\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0075\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0118\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0110\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0184\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.136877, Train Loss: 0.852547\n",
            "\n",
            "Start of epoch 425\n",
            "Training loss (for one batch) at step 0: 0.0021\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0009\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0014\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0011\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0183\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0200\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0090\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0206\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0174\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0071\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0137\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0054\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0182\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0041\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0099\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0269\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0104\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0059\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0074\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0049\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0101\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0194\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0201\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.105233, Train Loss: 0.845427\n",
            "\n",
            "Start of epoch 426\n",
            "Training loss (for one batch) at step 0: 0.0010\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0008\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0016\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0005\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0181\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0111\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0228\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0140\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0091\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0112\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0084\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0045\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0079\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0093\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0087\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0070\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0091\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0073\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0126\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0089\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0188\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0110\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0081\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.044194, Train Loss: 0.808848\n",
            "\n",
            "Start of epoch 427\n",
            "Training loss (for one batch) at step 0: 0.0012\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0010\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0009\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0006\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0155\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0095\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0130\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0141\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0179\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0068\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0144\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0060\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0107\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0097\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0220\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0080\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0077\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0073\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0093\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0125\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0088\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0100\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0110\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.056041, Train Loss: 0.805693\n",
            "\n",
            "Start of epoch 428\n",
            "Training loss (for one batch) at step 0: 0.0016\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0009\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0019\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0010\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0147\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0098\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0137\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0154\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0146\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0197\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0096\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0070\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0077\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0054\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0122\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0095\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0088\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0063\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0097\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0068\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0092\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0059\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0144\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.029856, Train Loss: 0.853185\n",
            "\n",
            "Start of epoch 429\n",
            "Training loss (for one batch) at step 0: 0.0012\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0008\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0019\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0007\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0193\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0136\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0091\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0197\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0201\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0074\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0096\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0048\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0094\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0069\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0107\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0081\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0124\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0063\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0117\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0047\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0100\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0079\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0206\n",
            "Seen so far: 28288 samples\n",
            "INFO:tensorflow:Assets written to: ./saved_model/model-429/assets\n",
            "\n",
            "Loss: 1.979825, Train Loss: 0.797531\n",
            "\n",
            "Start of epoch 430\n",
            "Training loss (for one batch) at step 0: 0.0015\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0005\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0011\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0019\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0153\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0241\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0115\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0180\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0199\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0055\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0076\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0042\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0099\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0073\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0090\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0070\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0090\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0076\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0098\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0050\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0084\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0089\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0079\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.994839, Train Loss: 0.808528\n",
            "\n",
            "Start of epoch 431\n",
            "Training loss (for one batch) at step 0: 0.0026\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0016\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0019\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0006\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0180\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0125\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0133\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0095\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0178\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0123\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0098\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0045\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0091\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0046\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0102\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0074\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0098\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0173\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0080\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0071\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0106\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0071\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0106\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.987146, Train Loss: 0.760413\n",
            "\n",
            "Start of epoch 432\n",
            "Training loss (for one batch) at step 0: 0.0018\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0010\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0009\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0009\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0159\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0128\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0218\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0118\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0151\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0060\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0117\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0089\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0082\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0065\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0098\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0078\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0087\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0090\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0103\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0137\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0113\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0087\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0106\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.055424, Train Loss: 0.789877\n",
            "\n",
            "Start of epoch 433\n",
            "Training loss (for one batch) at step 0: 0.0017\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0011\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0016\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0005\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0224\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0119\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0244\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0144\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0143\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0099\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0097\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0050\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0100\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0095\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0108\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0080\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0086\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0062\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0104\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0054\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0090\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0088\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0122\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.051955, Train Loss: 0.797848\n",
            "\n",
            "Start of epoch 434\n",
            "Training loss (for one batch) at step 0: 0.0022\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0006\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0013\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0004\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0167\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0109\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0188\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0200\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0148\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0104\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0079\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0073\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0085\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0069\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0108\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0074\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0132\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0174\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0116\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0081\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0099\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0085\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0139\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.062436, Train Loss: 0.797230\n",
            "\n",
            "Start of epoch 435\n",
            "Training loss (for one batch) at step 0: 0.0013\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0004\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0009\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0011\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0138\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0150\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0097\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0190\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0159\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0082\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0085\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0047\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0107\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0077\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0092\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0087\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0101\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0087\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0103\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0049\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0108\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0098\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0073\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.928515, Train Loss: 0.810001\n",
            "\n",
            "Start of epoch 436\n",
            "Training loss (for one batch) at step 0: 0.0028\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0011\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0012\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0008\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0175\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0222\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0082\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0192\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0141\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0090\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0084\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0048\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0103\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0078\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0107\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0110\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0080\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0092\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0117\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0065\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0108\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0096\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0138\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.027897, Train Loss: 0.796347\n",
            "\n",
            "Start of epoch 437\n",
            "Training loss (for one batch) at step 0: 0.0026\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0006\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0018\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0010\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0133\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0111\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0147\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0209\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0140\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0072\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0062\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0032\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0082\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0048\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0096\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0082\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0101\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0073\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0104\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0056\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0091\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0124\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0072\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.966945, Train Loss: 0.838724\n",
            "\n",
            "Start of epoch 438\n",
            "Training loss (for one batch) at step 0: 0.0011\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0005\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0020\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0006\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0209\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0115\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0103\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0122\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0171\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0205\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0117\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0068\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0086\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0057\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0178\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0086\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0105\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0046\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0079\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0079\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0088\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0097\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0095\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.998328, Train Loss: 0.816679\n",
            "\n",
            "Start of epoch 439\n",
            "Training loss (for one batch) at step 0: 0.0018\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0013\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0018\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0005\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0140\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0117\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0176\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0132\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0174\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0066\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0079\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0070\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0085\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0070\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0089\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0179\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0092\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0102\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0061\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0063\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0200\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0085\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0111\n",
            "Seen so far: 28288 samples\n",
            "INFO:tensorflow:Assets written to: ./saved_model/model-439/assets\n",
            "\n",
            "Loss: 2.033086, Train Loss: 0.803329\n",
            "\n",
            "Start of epoch 440\n",
            "Training loss (for one batch) at step 0: 0.0017\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0012\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0021\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0011\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0149\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0102\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0082\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0144\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0209\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0108\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0097\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0068\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0085\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0053\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0292\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0153\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0105\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0079\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0121\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0075\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0087\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0054\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0118\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.018882, Train Loss: 0.781203\n",
            "\n",
            "Start of epoch 441\n",
            "Training loss (for one batch) at step 0: 0.0014\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0015\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0016\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0008\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0175\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0165\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0145\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0145\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0187\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0073\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0193\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0070\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0198\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0059\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0191\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0070\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0088\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0073\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0188\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0071\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0156\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0070\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0092\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.053341, Train Loss: 0.830902\n",
            "\n",
            "Start of epoch 442\n",
            "Training loss (for one batch) at step 0: 0.0013\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0010\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0012\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0007\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0165\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0268\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0117\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0260\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0215\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0095\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0103\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0073\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0111\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0067\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0109\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0079\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0069\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0097\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0084\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0061\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0091\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0214\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0096\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.038279, Train Loss: 0.784020\n",
            "\n",
            "Start of epoch 443\n",
            "Training loss (for one batch) at step 0: 0.0029\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0014\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0021\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0009\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0112\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0144\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0083\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0130\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0167\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0074\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0083\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0066\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0089\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0091\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0093\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0066\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0095\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0082\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0114\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0068\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0180\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0113\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0074\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.974421, Train Loss: 0.774148\n",
            "\n",
            "Start of epoch 444\n",
            "Training loss (for one batch) at step 0: 0.0016\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0009\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0015\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0008\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0189\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0114\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0134\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0109\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0161\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0181\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0088\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0065\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0095\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0138\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0101\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0073\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0092\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0179\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0075\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0042\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0082\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0081\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0087\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.042550, Train Loss: 0.795673\n",
            "\n",
            "Start of epoch 445\n",
            "Training loss (for one batch) at step 0: 0.0012\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0008\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0022\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0007\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0171\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0114\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0094\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0184\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0361\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0151\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0100\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0070\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0084\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0074\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0079\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0088\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0111\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0062\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0103\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0088\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0106\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0097\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0106\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.108043, Train Loss: 0.816710\n",
            "\n",
            "Start of epoch 446\n",
            "Training loss (for one batch) at step 0: 0.0010\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0007\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0018\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0015\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0164\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0136\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0103\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0176\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0147\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0105\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0093\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0053\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0076\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0069\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0193\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0077\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0088\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0074\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0112\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0052\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0180\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0065\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0092\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.088918, Train Loss: 0.820542\n",
            "\n",
            "Start of epoch 447\n",
            "Training loss (for one batch) at step 0: 0.0009\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0020\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0018\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0012\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0184\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0108\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0095\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0105\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0186\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0083\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0219\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0097\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0108\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0064\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0110\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0153\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0107\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0058\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0194\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0058\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0163\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0099\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0106\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.030769, Train Loss: 0.797710\n",
            "\n",
            "Start of epoch 448\n",
            "Training loss (for one batch) at step 0: 0.0015\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0009\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0014\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0008\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0134\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0149\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0161\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0121\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0195\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0085\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0111\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0050\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0092\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0104\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0146\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0075\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0092\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0075\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0098\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0079\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0096\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0095\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0092\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.036924, Train Loss: 0.797022\n",
            "\n",
            "Start of epoch 449\n",
            "Training loss (for one batch) at step 0: 0.0018\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0009\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0012\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0020\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0154\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0184\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0155\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0121\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0244\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0075\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0189\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0040\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0293\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0065\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0095\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0072\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0107\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0059\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0098\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0049\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0100\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0088\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0063\n",
            "Seen so far: 28288 samples\n",
            "INFO:tensorflow:Assets written to: ./saved_model/model-449/assets\n",
            "\n",
            "Loss: 2.091656, Train Loss: 0.831909\n",
            "\n",
            "Start of epoch 450\n",
            "Training loss (for one batch) at step 0: 0.0013\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0014\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0016\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0004\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0194\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0138\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0085\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0157\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0224\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0078\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0108\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0189\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0088\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0057\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0269\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0134\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0071\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0073\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0107\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0087\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0122\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0087\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0248\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.066936, Train Loss: 0.803519\n",
            "\n",
            "Start of epoch 451\n",
            "Training loss (for one batch) at step 0: 0.0019\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0010\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0018\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0008\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0149\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0234\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0127\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0228\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0187\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0081\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0092\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0055\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0097\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0053\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0077\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0088\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0092\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0086\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0130\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0047\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0083\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0196\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0125\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.035545, Train Loss: 0.756605\n",
            "\n",
            "Start of epoch 452\n",
            "Training loss (for one batch) at step 0: 0.0022\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0014\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0016\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0005\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0168\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0092\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0126\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0139\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0155\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0080\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0107\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0076\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0119\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0086\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0091\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0077\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0084\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0074\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0094\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0085\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0183\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0104\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0094\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.031708, Train Loss: 0.804238\n",
            "\n",
            "Start of epoch 453\n",
            "Training loss (for one batch) at step 0: 0.0021\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0015\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0013\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0011\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0153\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0119\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0118\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0151\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0156\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0080\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0086\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0085\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0085\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0063\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0192\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0075\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0096\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0073\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0091\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0082\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0097\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0072\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0110\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.965976, Train Loss: 0.821178\n",
            "\n",
            "Start of epoch 454\n",
            "Training loss (for one batch) at step 0: 0.0010\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0005\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0017\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0007\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0247\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0098\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0114\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0248\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0183\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0081\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0100\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0093\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0208\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0082\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0084\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0099\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0087\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0058\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0288\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0091\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0085\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0116\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0080\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.034998, Train Loss: 0.806935\n",
            "\n",
            "Start of epoch 455\n",
            "Training loss (for one batch) at step 0: 0.0014\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0019\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0016\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0008\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0170\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0115\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0073\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0180\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0125\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0092\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0263\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0060\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0085\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0073\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0070\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0078\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0098\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0072\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0095\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0051\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0086\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0088\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0073\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.989955, Train Loss: 0.818239\n",
            "\n",
            "Start of epoch 456\n",
            "Training loss (for one batch) at step 0: 0.0012\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0019\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0014\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0009\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0170\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0135\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0159\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0139\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0156\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0082\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0093\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0155\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0099\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0085\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0116\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0065\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0094\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0084\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0081\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0042\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0075\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0110\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0112\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.033366, Train Loss: 0.783232\n",
            "\n",
            "Start of epoch 457\n",
            "Training loss (for one batch) at step 0: 0.0006\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0009\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0019\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0011\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0114\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0128\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0094\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0094\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0181\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0089\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0131\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0156\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0091\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0064\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0114\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0066\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0076\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0077\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0102\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0144\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0090\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0087\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0094\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.054929, Train Loss: 0.837583\n",
            "\n",
            "Start of epoch 458\n",
            "Training loss (for one batch) at step 0: 0.0013\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0010\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0014\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0008\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0130\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0128\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0066\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0161\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0146\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0056\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0206\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0050\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0100\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0051\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0097\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0183\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0075\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0085\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0113\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0064\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0096\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0097\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0071\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.973790, Train Loss: 0.781733\n",
            "\n",
            "Start of epoch 459\n",
            "Training loss (for one batch) at step 0: 0.0009\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0005\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0014\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0017\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0121\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0145\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0360\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0112\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0147\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0082\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0100\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0067\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0086\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0081\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0089\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0071\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0115\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0072\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0083\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0067\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0112\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0084\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0103\n",
            "Seen so far: 28288 samples\n",
            "INFO:tensorflow:Assets written to: ./saved_model/model-459/assets\n",
            "\n",
            "Loss: 2.086798, Train Loss: 0.789743\n",
            "\n",
            "Start of epoch 460\n",
            "Training loss (for one batch) at step 0: 0.0021\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0015\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0014\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0009\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0215\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0104\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0145\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0195\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0260\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0062\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0073\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0069\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0077\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0063\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0190\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0200\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0105\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0066\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0115\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0110\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0096\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0063\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0108\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.131644, Train Loss: 0.807979\n",
            "\n",
            "Start of epoch 461\n",
            "Training loss (for one batch) at step 0: 0.0012\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0006\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0014\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0007\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0193\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0163\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0093\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0206\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0158\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0068\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0166\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0118\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0105\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0054\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0120\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0075\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0108\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0177\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0074\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0076\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0083\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0098\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0098\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.021028, Train Loss: 0.813006\n",
            "\n",
            "Start of epoch 462\n",
            "Training loss (for one batch) at step 0: 0.0017\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0005\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0014\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0011\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0192\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0172\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0121\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0144\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0141\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0072\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0111\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0050\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0077\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0094\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0094\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0155\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0109\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0072\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0111\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0075\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0113\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0125\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0097\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.041499, Train Loss: 0.812485\n",
            "\n",
            "Start of epoch 463\n",
            "Training loss (for one batch) at step 0: 0.0012\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0005\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0022\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0010\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0169\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0103\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0076\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0134\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0160\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0070\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0156\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0071\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0105\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0057\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0083\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0232\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0093\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0078\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0177\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0065\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0091\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0069\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0122\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.978253, Train Loss: 0.794372\n",
            "\n",
            "Start of epoch 464\n",
            "Training loss (for one batch) at step 0: 0.0009\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0008\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0023\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0007\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0170\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0131\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0085\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0191\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0181\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0069\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0087\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0061\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0125\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0058\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0080\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0083\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0098\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0068\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0110\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0047\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0086\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0120\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0112\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.039579, Train Loss: 0.776041\n",
            "\n",
            "Start of epoch 465\n",
            "Training loss (for one batch) at step 0: 0.0027\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0011\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0018\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0010\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0219\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0108\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0070\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0160\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0229\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0089\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0118\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0079\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0119\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0116\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0098\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0065\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0091\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0064\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0098\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0064\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0085\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0117\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0081\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.976289, Train Loss: 0.783376\n",
            "\n",
            "Start of epoch 466\n",
            "Training loss (for one batch) at step 0: 0.0011\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0019\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0013\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0005\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0192\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0102\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0132\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0194\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0131\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0065\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0095\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0053\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0091\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0050\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0093\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0106\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0113\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0072\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0108\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0060\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0191\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0191\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0076\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.042994, Train Loss: 0.830746\n",
            "\n",
            "Start of epoch 467\n",
            "Training loss (for one batch) at step 0: 0.0008\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0011\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0014\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0010\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0154\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0152\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0090\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0230\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0156\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0084\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0104\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0040\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0124\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0088\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0130\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0083\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0082\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0086\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0084\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0064\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0150\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0194\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0118\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.046110, Train Loss: 0.790017\n",
            "\n",
            "Start of epoch 468\n",
            "Training loss (for one batch) at step 0: 0.0014\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0009\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0013\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0004\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0202\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0136\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0074\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0161\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0143\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0059\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0084\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0051\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0074\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0065\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0067\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0107\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0099\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0071\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0098\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0054\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0112\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0116\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0160\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.896480, Train Loss: 0.770416\n",
            "\n",
            "Start of epoch 469\n",
            "Training loss (for one batch) at step 0: 0.0010\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0010\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0039\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0008\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0134\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0150\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0138\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0184\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0151\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0073\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0082\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0045\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0108\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0077\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0089\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0084\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0073\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0079\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0183\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0053\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0093\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0131\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0111\n",
            "Seen so far: 28288 samples\n",
            "INFO:tensorflow:Assets written to: ./saved_model/model-469/assets\n",
            "\n",
            "Loss: 1.984317, Train Loss: 0.768507\n",
            "\n",
            "Start of epoch 470\n",
            "Training loss (for one batch) at step 0: 0.0013\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0015\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0017\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0010\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0165\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0092\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0239\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0182\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0169\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0064\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0076\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0043\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0092\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0038\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0123\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0076\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0088\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0081\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0102\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0052\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0084\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0093\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0083\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.004455, Train Loss: 0.763561\n",
            "\n",
            "Start of epoch 471\n",
            "Training loss (for one batch) at step 0: 0.0041\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0011\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0017\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0007\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0149\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0152\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0086\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0157\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0158\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0084\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0231\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0048\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0090\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0059\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0095\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0098\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0094\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0090\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0097\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0058\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0099\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0119\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0217\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.049594, Train Loss: 0.806939\n",
            "\n",
            "Start of epoch 472\n",
            "Training loss (for one batch) at step 0: 0.0015\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0017\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0016\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0008\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0130\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0170\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0140\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0200\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0169\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0071\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0118\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0046\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0179\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0060\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0125\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0105\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0077\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0120\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0186\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0050\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0221\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0104\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0093\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.012801, Train Loss: 0.801875\n",
            "\n",
            "Start of epoch 473\n",
            "Training loss (for one batch) at step 0: 0.0014\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0008\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0024\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0004\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0157\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0113\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0070\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0157\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0268\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0085\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0076\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0057\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0083\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0064\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0090\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0164\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0140\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0062\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0105\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0042\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0089\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0057\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0088\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.997956, Train Loss: 0.854504\n",
            "\n",
            "Start of epoch 474\n",
            "Training loss (for one batch) at step 0: 0.0016\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0012\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0019\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0011\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0185\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0120\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0169\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0128\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0174\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0113\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0114\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0046\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0101\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0083\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0101\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0075\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0099\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0087\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0074\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0076\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0213\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0145\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0117\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.008741, Train Loss: 0.826850\n",
            "\n",
            "Start of epoch 475\n",
            "Training loss (for one batch) at step 0: 0.0010\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0011\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0009\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0005\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0143\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0102\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0101\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0294\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0169\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0101\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0069\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0048\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0115\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0044\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0208\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0080\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0111\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0061\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0107\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0069\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0090\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0108\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0098\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.982709, Train Loss: 0.805901\n",
            "\n",
            "Start of epoch 476\n",
            "Training loss (for one batch) at step 0: 0.0018\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0017\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0013\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0010\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0175\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0087\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0068\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0144\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0156\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0078\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0081\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0072\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0097\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0064\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0194\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0068\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0118\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0070\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0202\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0065\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0187\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0090\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0105\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.057887, Train Loss: 0.839643\n",
            "\n",
            "Start of epoch 477\n",
            "Training loss (for one batch) at step 0: 0.0012\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0022\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0019\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0013\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0202\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0131\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0094\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0176\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0144\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0085\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0093\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0061\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0104\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0160\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0127\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0073\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0091\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0070\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0073\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0074\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0178\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0052\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0168\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.018991, Train Loss: 0.823566\n",
            "\n",
            "Start of epoch 478\n",
            "Training loss (for one batch) at step 0: 0.0015\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0006\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0022\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0005\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0202\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0106\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0103\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0188\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0194\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0080\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0088\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0052\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0207\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0076\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0100\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0076\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0096\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0075\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0097\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0069\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0099\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0076\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0111\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.993869, Train Loss: 0.796006\n",
            "\n",
            "Start of epoch 479\n",
            "Training loss (for one batch) at step 0: 0.0010\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0006\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0020\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0008\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0125\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0103\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0090\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0176\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0257\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0094\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0093\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0053\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0113\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0100\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0084\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0073\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0095\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0077\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0124\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0079\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0190\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0096\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0106\n",
            "Seen so far: 28288 samples\n",
            "INFO:tensorflow:Assets written to: ./saved_model/model-479/assets\n",
            "\n",
            "Loss: 2.013740, Train Loss: 0.828434\n",
            "\n",
            "Start of epoch 480\n",
            "Training loss (for one batch) at step 0: 0.0008\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0011\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0014\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0009\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0141\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0167\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0112\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0164\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0140\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0193\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0054\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0064\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0080\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0119\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0117\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0083\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0089\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0092\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0121\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0042\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0095\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0118\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0083\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.995925, Train Loss: 0.814265\n",
            "\n",
            "Start of epoch 481\n",
            "Training loss (for one batch) at step 0: 0.0009\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0013\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0014\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0008\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0135\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0158\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0102\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0164\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0163\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0085\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0080\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0053\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0078\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0096\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0098\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0276\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0068\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0077\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0083\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0037\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0093\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0096\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0088\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.943969, Train Loss: 0.821035\n",
            "\n",
            "Start of epoch 482\n",
            "Training loss (for one batch) at step 0: 0.0017\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0006\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0017\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0006\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0180\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0132\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0260\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0251\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0211\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0058\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0089\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0070\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0092\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0058\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0097\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0073\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0088\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0271\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0109\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0079\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0102\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0128\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0089\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.135354, Train Loss: 0.825451\n",
            "\n",
            "Start of epoch 483\n",
            "Training loss (for one batch) at step 0: 0.0015\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0011\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0013\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0010\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0128\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0140\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0104\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0197\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0162\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0071\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0101\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0054\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0140\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0067\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0123\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0158\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0212\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0127\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0099\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0166\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0209\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0103\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0091\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.947924, Train Loss: 0.800145\n",
            "\n",
            "Start of epoch 484\n",
            "Training loss (for one batch) at step 0: 0.0014\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0006\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0016\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0015\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0185\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0083\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0096\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0126\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0125\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0079\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0204\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0058\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0066\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0093\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0249\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0075\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0111\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0069\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0109\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0151\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0103\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0063\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0082\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.018054, Train Loss: 0.811724\n",
            "\n",
            "Start of epoch 485\n",
            "Training loss (for one batch) at step 0: 0.0012\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0006\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0017\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0016\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0127\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0133\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0120\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0176\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0196\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0089\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0095\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0070\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0122\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0119\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0081\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0081\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0117\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0084\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0100\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0166\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0174\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0184\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0201\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.015645, Train Loss: 0.772777\n",
            "\n",
            "Start of epoch 486\n",
            "Training loss (for one batch) at step 0: 0.0022\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0015\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0017\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0008\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0187\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0122\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0111\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0162\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0147\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0083\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0094\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0065\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0173\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0079\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0079\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0076\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0094\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0076\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0088\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0048\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0108\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0085\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0117\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.902269, Train Loss: 0.772256\n",
            "\n",
            "Start of epoch 487\n",
            "Training loss (for one batch) at step 0: 0.0011\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0007\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0018\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0010\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0130\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0176\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0109\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0160\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0137\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0069\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0080\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0081\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0204\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0066\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0176\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0122\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0101\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0385\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0148\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0109\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0101\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0079\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0174\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.979167, Train Loss: 0.768978\n",
            "\n",
            "Start of epoch 488\n",
            "Training loss (for one batch) at step 0: 0.0011\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0007\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0015\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0005\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0139\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0133\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0067\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0160\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0173\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0065\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0100\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0072\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0112\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0072\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0093\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0088\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0099\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0198\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0077\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0055\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0088\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0153\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0089\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.988344, Train Loss: 0.790054\n",
            "\n",
            "Start of epoch 489\n",
            "Training loss (for one batch) at step 0: 0.0013\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0004\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0017\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0007\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0116\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0145\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0090\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0154\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0198\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0093\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0213\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0069\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0093\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0076\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0060\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0077\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0119\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0054\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0116\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0176\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0176\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0059\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0121\n",
            "Seen so far: 28288 samples\n",
            "INFO:tensorflow:Assets written to: ./saved_model/model-489/assets\n",
            "\n",
            "Loss: 1.975752, Train Loss: 0.784346\n",
            "\n",
            "Start of epoch 490\n",
            "Training loss (for one batch) at step 0: 0.0011\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0006\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0014\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0013\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0125\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0141\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0125\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0147\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0140\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0086\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0082\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0082\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0141\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0065\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0073\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0072\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0087\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0064\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0088\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0148\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0086\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0103\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0125\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.032659, Train Loss: 0.797532\n",
            "\n",
            "Start of epoch 491\n",
            "Training loss (for one batch) at step 0: 0.0005\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0011\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0016\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0007\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0124\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0126\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0103\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0254\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0139\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0061\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0101\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0065\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0086\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0087\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0066\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0073\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0102\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0078\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0115\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0058\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0076\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0064\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0108\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.998215, Train Loss: 0.803601\n",
            "\n",
            "Start of epoch 492\n",
            "Training loss (for one batch) at step 0: 0.0015\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0004\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0021\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0009\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0191\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0094\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0301\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0162\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0192\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0063\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0111\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0065\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0092\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0053\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0084\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0056\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0117\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0058\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0106\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0080\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0075\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0218\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0106\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.024318, Train Loss: 0.777421\n",
            "\n",
            "Start of epoch 493\n",
            "Training loss (for one batch) at step 0: 0.0021\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0005\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0013\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0015\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0136\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0112\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0083\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0133\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0209\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0065\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0099\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0048\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0125\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0067\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0166\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0080\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0083\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0095\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0118\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0086\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0103\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0152\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0107\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.973157, Train Loss: 0.776510\n",
            "\n",
            "Start of epoch 494\n",
            "Training loss (for one batch) at step 0: 0.0013\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0015\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0010\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0005\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0174\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0118\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0109\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0197\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0201\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0090\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0100\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0084\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0091\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0058\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0197\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0070\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0078\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0050\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0267\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0063\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0084\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0083\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0118\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.031092, Train Loss: 0.827598\n",
            "\n",
            "Start of epoch 495\n",
            "Training loss (for one batch) at step 0: 0.0012\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0006\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0016\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0008\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0124\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0212\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0103\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0193\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0157\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0053\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0203\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0056\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0199\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0059\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0083\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0168\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0093\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0064\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0126\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0066\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0103\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0073\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0091\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.975452, Train Loss: 0.769513\n",
            "\n",
            "Start of epoch 496\n",
            "Training loss (for one batch) at step 0: 0.0011\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0005\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0014\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0005\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0132\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0117\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0139\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0130\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0175\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0081\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0091\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0052\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0086\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0163\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0111\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0174\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0096\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0065\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0080\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0046\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0092\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0108\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0078\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 2.038392, Train Loss: 0.763846\n",
            "\n",
            "Start of epoch 497\n",
            "Training loss (for one batch) at step 0: 0.0018\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0014\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0009\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0008\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0141\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0120\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0116\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0151\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0166\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0039\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0169\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0052\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0130\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0069\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0063\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0070\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0071\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0139\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0119\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0191\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0121\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0077\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0101\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.935747, Train Loss: 0.766153\n",
            "\n",
            "Start of epoch 498\n",
            "Training loss (for one batch) at step 0: 0.0026\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0013\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0016\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0012\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0110\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0141\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0094\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0164\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0162\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0097\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0113\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0042\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0081\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0061\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0103\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0088\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0089\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0062\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0090\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0056\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0276\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0199\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0185\n",
            "Seen so far: 28288 samples\n",
            "\n",
            "Loss: 1.976343, Train Loss: 0.799438\n",
            "\n",
            "Start of epoch 499\n",
            "Training loss (for one batch) at step 0: 0.0011\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 10: 0.0008\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 20: 0.0013\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 30: 0.0009\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 40: 0.0159\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 50: 0.0115\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 60: 0.0105\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 70: 0.0108\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 80: 0.0138\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 90: 0.0079\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 100: 0.0090\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 110: 0.0058\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 120: 0.0104\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 130: 0.0106\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 140: 0.0082\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 150: 0.0067\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 160: 0.0127\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 170: 0.0073\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 180: 0.0106\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 190: 0.0059\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 200: 0.0187\n",
            "Seen so far: 25728 samples\n",
            "Training loss (for one batch) at step 210: 0.0100\n",
            "Seen so far: 27008 samples\n",
            "Training loss (for one batch) at step 220: 0.0202\n",
            "Seen so far: 28288 samples\n",
            "INFO:tensorflow:Assets written to: ./saved_model/model-499/assets\n",
            "\n",
            "Loss: 2.033439, Train Loss: 0.773183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r ./saved_model /content/drive/Shareddrives/Sam/"
      ],
      "metadata": {
        "id": "TjuB0l7XQ8Mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_loss_values)\n",
        "plt.plot(test_loss_values)\n",
        "plt.ylim(0, 10)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "DR5dwoI2Cetu",
        "outputId": "af2e8949-ac9c-4090-ea32-4d52c4794eb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}